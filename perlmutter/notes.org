* Baseline point to point performance

** OSU microbenchmarks (http://mvapich.cse.ohio-state.edu/benchmarks/)

Use CUDA-aware MPI for device-to-device transfers

*** Single node performance

**** ~osu_bw~ (Uni-directional bandwidth)

Single message at a time (~-W 1~), processes bind to different GPUs.
#+begin_src 
srun -n 2  ../../../get_local_rank ./osu_bw -W 1 -d cuda -m 1:67108864 D D
#+end_src

Measures uni-directional bandwidth
#+begin_src c
  if (rank == 0) {
    start = time();
    req = Isend(buf, size, 1);
    Wait(req);
    // Wait for confirmation
    Recv(rbuf, 1, 1);
    end = time();
    bw = size / (end - start);
  } else {
    req = Irecv(buf, size, 0);
    Wait(req);
    Send(sbuf, 1, 0);
  }
#+end_src


**** ~osu_bibw~ (Bi-directional bandwidth)

#+begin_src
srun -n 2  ../../../get_local_rank ./osu_bibw -W 1 -d cuda -m 1:67108864 D D
#+end_src
Measures bi-directional bandwidth
#+begin_src c
  if (rank == 0) {
    start = time();
    req1 = Irecv(rbuf, size, 1);
    req2 = Isend(buf, size, 1);
    Waitall(req1, req2);
    end = time();
    bw = 2*size / (end - start);
  } else {
    req1 = Irecv(buf, size, 0);
    req2 = Isend(sbuf, size, 0);
    Waitall(req1, req2);
  }
#+end_src


**** ~osu_latency~ (Message latency)

#+begin_src
srun -n 2  ../../../get_local_rank ./osu_latency -d cuda -m 1:67108864 D D
#+end_src


*** Multi node performance

**** ~osu_bw~

#+begin_src
srun -n 2 --tasks-per-node 1 ../../../get_local_rank ./osu_bw -W 1 -d cuda -m 1:67108864 D D
#+end_src


**** ~osu_bibw~
#+begin_src
srun -n 2 --tasks-per-node 1 ../../../get_local_rank ./osu_bibw -W 1 -d cuda -m 1:67108864 D D
#+end_src


**** ~osu_latency~

#+begin_src
srun -n 2 --tasks-per-node 1 ../../../get_local_rank ./osu_latency -d cuda -m 1:67108864 D D
#+end_src

Measures half ping-pong latency
#+begin_src c
  if (rank == 0) {
    start = time();
    Send(buf, size, 1);
    Recv(rbuf, size, 1);
    end = time();
    latency = (end - start) / 2;
   } else {
    Recv(buf, size, 0);
    Send(sbuf, size, 0);
   }
#+end_src


** UCX-Py microbenchmarks

Using ucx-py's microbenchmarks. These measure bi-directional bandwidth
(with blocking sends).

#+begin_src py
start = time()
blocking_send(ep, buf)
blocking_recv(ep, buf)
end = time()
bw = 2 * buf.nbytes / (end - start)
#+end_src

*** Single node performance

**** ~send-recv-core.py~
#+begin_src sh
  for pow in $(seq 0 26); do
     nbytes=$((2**pow))
     val=$(python send-recv-core.py -o cupy --reuse-alloc \
           -d 1 -e 0 --n-iter 30 -b 0 -c 1 -n $nbytes | grep Median | cut -d \| -f 2)
     echo $nbytes $val
  done
#+end_src

**** ~send-recv.py~

#+begin_src sh
  for pow in $(seq 0 26); do
     nbytes=$((2**pow))
     val=$(python send-recv.py -o cupy --reuse-alloc \
           -d 1 -e 0 --n-iter 30 -b 0 -c 1 -n $nbytes | grep Median | cut -d \| -f 2)
     echo $nbytes $val
  done
#+end_src


*** Multi node performance

**** ~send-recv-core.py~

~srun --nodes 2 --tasks-per-node 1 ./foo.sh~
#+begin_src sh
  #!/bin/bash                                                                                                                                                                                                            

  for pow in $(seq 0 26); do
      nbytes=$((2**pow))
      if [[ $SLURM_PROCID == 0 ]]; then
          val=$(python send-recv-core.py  -b 0 -o cupy --reuse-alloc -d 0 --server-only -p 57904 -n $nbytes --n-iter 30 | grep Median | cut -d \| -f 2)
      else
          val=$(python send-recv-core.py -c 0 -o cupy --reuse-alloc -p 57904 --client-only -s nid003476 -e 0 --n-iter 30 -n $nbytes | grep Median | cut -d \| -f 2)
          echo $nbytes $val
      fi
  done
#+end_src

**** ~send-recv.py~

~srun --nodes 2 --tasks-per-node 1 ./foo.sh~
#+begin_src sh
  #!/bin/bash                                                                                                                                                                                                            

  for pow in $(seq 0 26); do
      nbytes=$((2**pow))
      if [[ $SLURM_PROCID == 0 ]]; then
          val=$(python send-recv.py  -b 0 -o cupy --reuse-alloc -d 0 --server-only -p 57904 -n $nbytes --n-iter 30 | grep Median | cut -d \| -f 2)
      else
          val=$(python send-recv.py -c 0 -o cupy --reuse-alloc -p 57904 --client-only -s nid003476 -e 0 --n-iter 30 -n $nbytes | grep Median | cut -d \| -f 2)
          echo $nbytes $val
      fi
  done
#+end_src


* TODO dask merge performance

Analyse data from most recent set of -c 60_000_000 runs to produce
throughput plots.

* TODO dask shuffle and transpose performance

Kick these runs off.
